ABOUT THIS DOCUMENT: This file are the lab notes of Joe Strout, a graduate student working in the vision lab under Ross Beveridge and Bruce Draper in 2019-2020.  The notes mainly reflect development of Diana 2.0, an avatar that communicates with the human user via speech and gestures, as part of the Communicating with Computers (CwC) project.

Entries are dated (in month/day/year) form and arranged chronologically, newest entries at the end of the document.

5/09/19
=======
I'm working on an internal representation of speech, to act as an intermediary between the raw English text or parse tree from some particular parsing tool, and the rest of the system (cognition, planning, knowledge representation, response generation, etc.).

This will almost certainly be some sort of semantic network.  Implementation details are still TBD.

While most inputs from the user are likely to be commands or queries, we want to be able to be able to capture other types of input, too.

Major communication-act types:
	Confirmation (e.g.: yes, correct, that's right)
	Denial (no)
	Query (who/what/when/why/how)
	Command/Request
	Assert/Inform
	Emote (wow, argh)
	Phatic/Backchannel (hello, please, thank you, you're welcome, ah, I see, OK)

Almost all commands (and most queries) will involve actions -- either something that has happened, or something the user wants to happen.  Actions also show up as subclauses in other contexts, such as identifying objects (e.g. "the block you moved before").  So we need to give special attention to describing actions.  I'm basing the following on standard linguistic themes, but lumping together some that seem like needless distinctions to me, and adding Activity (which was oddly absent from the source I used).

Themes (slots on an action):
	Subject (Agent, Experiencer, Force/Natural Cause)
	Activity (verb)
	Object (Patient, Stimulus, Theme)
	Instrument (with, using, by means of)
	Location
	Direction (Goal)
	Recipient (aka indirect object)
	Source/Origin
	Time
	Beneficiary (may not be needed for Diana)
	Manner (ditto)
	Purpose
	Cause

5/10/19
=======
I've started on a parser.  I considered using StanfordNLP, but wasn't very impressed with the quality of its parsing when I used it on my CS540 project.  AllenNLP looks better, but it's an awfully heavy install.  I think we can bang out a mostly hand-coded parser that will do as well for the range of inputs we'll be likely to give it.

To that end, I started a "NLPEx1" project (in Visual Studio for Mac).  It leverages data extracted from the SEMCOR 3.0 data set (downloaded via http://www.nltk.org/nltk_data/).  For every word (and set phrase), I collected all the instances in the corpus and counted its usage as various parts of speech (POS).  Then I output a simple file with the word, and the POS frequencies, normalized to 1.  The data has a lot of extra cruft in it, but for the most part the words I've looked at seem good (with a few exceptions).

So here's the big picture of the parser so far:

1. Replace set phrases, e.g. "thank you" -> "thank_you".
2. Split into words, with parallel arrays for POS and dependencies (i.e., what word each word modifies).
3. Assign initial parts of speech from the (tweaked) SEMCOR dictionary.
4. Repeatedly apply a set of ordered heuristics, that define dependencies and/or change POS based on simple patterns (e.g., an adjective before a noun modifies that noun).

That's pretty much it.  I'm not attempting to do a *complete* parse of the input; it's fine if multiple phrases remain floating at the root.  Indeed, it's impossible to reliably disambiguate from syntax alone; consider:

	A. I looked up the hallway.
	B. I looked up the address.

Syntactically identical, but in one case we have essentially a compound verb ("look_up") while in the second, "up" is an ordinary preposition.  Actually this is an unfortunate example, since our step 3 replaces this with looked_up before we even start parsing.  Maybe this is better:

	A. Put the red box on the table.
	B. Grab the red box on the table.
	
In A, the prepositional phrase modifies "put;" in B it modifies "box."  There is no way to disambiguate this without delving into what "put" and "grab" mean.  We could have tables of such information, but I think it'll be easier to just let the put/grab handlers work it out.

So the parsing we're doing pretty much locates noun phrases and prepositional phrases, and I think that's good enough.

I currently have two forms of string output.  One has the words in order, with annotations for POS and dependency, like so:

[put(VB:-1) a(DT:3) yellow(JJ:3) block(NN:0) on(IN:-1) the(DT:7) green(JJ:7) one(NN:4)]

This is easily read as "put a yellow block on the green one."  The other format is tree form, with dependents under the head word they depend on:

[VB[put NN[block DT[a] JJ[yellow]]] IN[on NN[one DT[the] JJ[green]]]]

This format is a little harder to read, but it better represents patterns we might match against, such as [VB[put NN[$1] IN[$2]] in this case.  I'm concerned, however, that this tree format loses information on word order that may be important.  For example, "is there a red block" and "there is a red block" both come out as

[VB[is RB[there] NN[block DT[a] JJ[red]]]]

in tree form.  So I may need to somehow preserve that word order, for cases where it matters.


5/13/19
=======
I haven't thought of a better order-preserving tree format than:

[VB[RB[there] is NN[DT[a] JJ[red] block]]]

It's a little awkward in that the POS can be so far separated from the actual word it's talking about (e.g. VB and "is" in this case).  But it'll do.

Basic parser is doing nicely now.  I've adapted the UnitTest class from the BlocksWorld project, and added some unit tests verifying the parse (in tree format) of a variety of inputs.  I expect we will eventually need quite a lot of those, but for now it's time to push on to interpretation.

That means building a semantic-network representation of the input (first we must understand the input, *then* we act or respond).  And here we face an implementation choice: use a generic semnet asset, like RDF?  Or a custom strongly-typed one, with properties for the various slots defined above?

Advantages of RDF: it's a standard; later researchers will more easily recognize it; it comes with utilities to read and write semnets in various formats.  Also supports querying and updating with SPARQL.

Advantage of custom classes: strong type-checking, probably simpler/clearer code.  Also less code overall (dotNetRDF is huge).

I'm going to go with custom classes for now, but try to encapsulate this decision as much as possible so we can swap out a different implementation later if we decide to do so.  


5/15/19
=======
Despite the claim in the previous entry, my first attempt at a semantic representation turned out to basically reinvent RDF.  I have a SemNet class that contains SemNodes linked by Relations (triplets), and an ontology that spells out a variety of attributes and meta-relations.  It's all very general but also very weakly typed.  Elegant, but hard to use, as I discovered as soon as I tried to actually use it.

I believe this was a misstep.  I'm going to back up and try again, with a series of more strongly-typed classes and enums.  I don't know that I'll be able to encapsulate this decision; a strongly-typed API is the whole point of it.  But let's try it and see how it goes.

OK, this feels much better.  More work will be needed to generalize it, but it's very nice to have strongly-typed representations to work with.  So now, the input:

	put a yellow block on the green one

gets parts of speech and dependencies assigned as:

	[put(VB:-1) a(DT:3) yellow(JJ:3) block(NN:0) on(IN:-1) the(DT:7) green(JJ:7) one(NN:4)]

which is represented in tree form as:

	[VB[put NN[DT[a] JJ[yellow] block]] IN[on NN[DT[the] JJ[green] one]]]

and this is grokked as the following ActionSpec:

	[Act:Put Obj:[any single Yellow block] Loc:[OnTopOf [the single Green one]]]

The latter is the string form of a data structure that actually represents all those parts in (strongly-typed) symbolic form, and would be easy for the avatar to carry out.


5/22/19
=======
Added a Communication class hierarchy, with subclasses for the seven subtypes of communication (command, query, etc.).  The main Grok method has been updated to grok both commands and phatic greetings/goodbyes.  So when you enter "hello", it now gets reduced to:

  --> Phatic(Greeting) : [hello(UH:-1)](Score=0)

I think this is about ready to integrate with the main BlocksWorld (Diana 2.0) Unity project.  There's still a lot more to be done on it, but the foundation is solid and it will already be a fine replacement for the current StanfordNLP-based language modules in there now.


5/23/19
=======
I've begun integration of the new parser module with the main BlocksWorld project.  So far, I have only a couple of phatic responses hooked up in the PleasantriesModule.


5/24/19
=======
I've added a CommandsModule, which processes direct commands from the user (grokked as ComCommand objects).  Right now you can tell the avatar "open your eyes" or "close your eyes", and it will do it (works for both Diana and SAM, though SAM does a better job of resting when his eyes are closed, and so he can no longer see where you are pointing).  Any other command (including, for example "close my eyes") and the avatar will respond "I can't."

Still need to do anaphora resolution, so that after saying "close your eyes" you could then say "open them."  And of course we will want many more commands, such as "raise your left hand" and "point where I point."


5/30/19
=======
As another example of how a traditional corpus fails to serve our needs well, I discovered today that the word "Point" in the SEMCOR database works out to these parts of speech:

Point:NNP=1.000
point:NN=0.970;VB=0.030

In other words, overwhelmingly a noun, even when capitalized (as in the start of a sentence).  That's because SEMCOR includes very few imperative statements.  Yet in a command context, "point" is almost always a verb (point at the red block, look where I point, etc.).  We really need a corpus of imperative and interrogative dialog.

But for now, I just added "point" to the list of words I have manually overridden in PartOfSpeech.cs.


6/01/19
=======
There's been a suggestion that we should replace the working DataStore implementation with an external dependency on Redis.  This would be a mistake, for a number of reasons:

- the C# implementation is already working perfectly
- the code is simple and trivial to maintain or extend (for example, I just added file logging and timestamping in about 15 minutes)
- an in-memory store will always be more performant than requesting data over the network, and in some cases we request values on every frame
- currently Diana can be built as a simple stand-alone app with no external dependencies; that will be impossible, even for a reduced set of cognitive modules, if the whole system is built on Redis
- David is already running into difficulties working out a cross-platform build system for Redis (a non-issue with Unity code)

I've seen so many projects flounder because they were assembled as a tottering structure of external libraries and dependencies.  Using a third-party library for some small, encapsulated piece of functionality that can easily be replaced (e.g. speech recognition or synthesis) is safe; building your whole architecture around it, in a way that can't be encapsulated, is costly and runs a serious risk of ruining the project.

But apparently we're going to discuss it at the lab meeting on Monday; I'll present the case for keeping our current approach there.


6/10/19
=======
After much discussion -- and Dave's discovery that Redis doesn't support Windows very well -- we finally decided to proceed with the current DataStore implementation, and a TCP socket interface for communicating with external processes.  Implemented that over the weekend, including a Python demo.


6/18/19
=======
I've been tasked with migrating KSIM (currently a standalone Windows executable) into Unity.  KSIM's job is mainly to read the Kinect sensor, and provide frames or skeleton data to clients that need it.  We should be able to do that in Unity.

The "normal" way of starting Kinect development appears to be using the Kinect for Windows SDK (https://www.microsoft.com/en-us/download/details.aspx?id=44561).  But Microsoft also provides a Unity plugin (http://go.microsoft.com/fwlink/?LinkID=513177).  I've downloaded the latter, and imported both:

	Kinect.2.0.1410.19000.unitypackage
	Kinect.Face.2.0.1410.19000.unitypackage

(the latter because I see that our KinectSensor.cs script in the KSIM project is using Microsoft.Kinect.Face).

Project still runs fine on my Mac.  So I'm going to check that in and call it a good start.
