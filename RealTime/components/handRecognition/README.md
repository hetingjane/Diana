# Overall structure
There are two classifiers: the `BaseClassifier` and `OneShotClassifier`. The `OneShotClassifier` inherits from the `BaseClassifier` to implement one shot learning functions. Both of them has the function of taking frame information (such as depth data) as input and generating byte array to send to fusion. The `_process()` and `_get_hand_recognition()` methods are overriden. Notice that there are two classes of `RealTimeHandRecognition` (in realtime_hand_recogition.py): the base one generates both label and probabilities and the inherited one shot one only generates feature vector. 

# `OneShotClassifier`
This class implements one shot learning functionalities. There are two threads: T1 as the main thread and T2 as the learning thread. The main idea behind this design is that T1 is always generating hand labels and probabilities to send to fusion, while the one-shot learning is handled in T2 without interfering with T1. The shared variables between two threads are a thread-safe queue, a `ForestStatus` instance, which keeps track of whether the forest is ready to be used and whether the forest is a fresh copy, and several event variables. In every case, the received depth data and skeleton data is added into the queue by T1, and T2 pops data from the queue to process. 

## Common Senarior
Most often no one-shot learning is in progress, therefore T2 is barely doing anything. T1 takes depth data from kinect and generates a feature vector, which is then used to search in the random forest to find its label. The result is packed to be sent to fusion. In this case, T2 pops data from the shared queue although nothing needs to be done.

## Learning phase
When kinect sends a learning signal (`writer_data_hand == b'learn'`), the learning process is initialited. In this case, the forest status is flipped so that it is not ready any more and the forest cannot be used to generate hand label. The `self.learning` variable in T1 is set to be true, and the posture 'learning' (index 35) is sent to fusion. T2 begins to process one-shot learning without interfering with T1 using the data popped from the shared queue. The exact details are described below. If the learning is successful, `self.event_vars.learn_complete_event` is set in T2, T1 can detect the event and will send a learning successful signal to fusion (posture index 36, 37, 38 depending on which one is learned); if not, `self.event_vars.learn_no_action_event` is set. In any case, T2 will reset forest status so that the forest is ready for use, and the `self.learning` variable in T1 is reset to be False.

## Disengage
When a user disengages, the forest needs to forget what it has learned. The `self.event_vars.load_forest_event` is set in T1 to indicate a loading event, and the actual loading is done in T2.

# `OneShotWorker` (handRecognition/RandomForest/threaded_one_shot.py)
This class handles the actual learning logic and loading of random forest. The most important thing is to determine what frames will be used as reference images for new gesture. When a learning signal is received from kinect, this class prepares for one shot learning. It will first determine whether the following depth images are actual meaningful hand gestures (not next to body or table). The logic is defined in `_is_gesture()`. It will not buffer the gestures until meaningful gestures appear. Then it will buffer `self.buffer_length` of gestures (keeping 1 frame for every 3 frames) for 3 seconds, and determine if the hand is still by calculating the variance of the palm center coordinates. If the hand is still, the 30 buffered frames from 3 seconds will be used as reference for new gesture. In this case, learning is successful. Otherwise, the learning is considered as failed. 

# `RandomForest` package
This package can be used to build a random forest instance. The methods are self-explainable, and one can reduce the number of trees or reduce the number of reference samples to make the model smaller. The forest is robust, so theorectically it does not require too many trees. Also, the code also has a legacy function of building random forest based on clips instead of single frames. This can be ignored since only frame features were used in practice. 